import asyncio
import aiohttp
import json
import re
import requests
import os
from typing import Dict, List, Optional, Tuple
from aiohttp import FormData

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
ACCOUNTS_FILE = "accounts.json"
COOKIES_DIR = "cookies"

class Resume:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ."""
    
    def __init__(self, hash: str, query: str, blacklist: List[str] = None):
        self.hash = hash
        self.query = query  # –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
        self.blacklist = blacklist or []  # –°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–∞–µ–º—ã—Ö —Å–ª–æ–≤/—Ñ—Ä–∞–∑

class Account:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–∫–∫–∞—É–Ω—Ç–æ–º –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç–∫–ª–∏–∫–æ–≤ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏."""
    
    def __init__(self, email: str, resumes: List[Resume]):
        self.email = email
        self.resumes = resumes
        self.cookies = {}
        self.is_token_being_updated = False
        self.load_cookies()

    def get_cookies_file_path(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫—É–∫–∞–º–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞."""
        return os.path.join(COOKIES_DIR, f"{self.email}.json")

    def load_cookies(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—É–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
        cookies_file = self.get_cookies_file_path()
        if os.path.exists(cookies_file):
            try:
                with open(cookies_file, "r", encoding="utf-8") as file:
                    data = json.load(file)
                    self.cookies = data.get("cookies", {})
            except (json.JSONDecodeError, FileNotFoundError):
                self.cookies = {}

    def update_cookies(self, cookies: Dict[str, str]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—É–∫–∏ –≤ –æ–±—ä–µ–∫—Ç–µ."""
        self.cookies.update(cookies)

    def save_cookies_to_file(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫—É–∫–∏ –≤ —Ñ–∞–π–ª."""
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É cookies, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(COOKIES_DIR, exist_ok=True)
        
        cookies_file = self.get_cookies_file_path()
        data = {"cookies": self.cookies}
        
        with open(cookies_file, "w", encoding="utf-8") as file:
            json.dump(data, file, indent=4)

    def prompt_cookies_update(self) -> None:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–æ–≤—ã–µ –∫—É–∫–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏—Ö."""
        self.is_token_being_updated = True
        new_cookies = parse_cookies(input(f"–í–≤–µ–¥–∏—Ç–µ –Ω–æ–≤—ã–µ –∫—É–∫–∏ –¥–ª—è {self.email}: "))
        self.update_cookies(new_cookies)
        self.save_cookies_to_file()
        self.is_token_being_updated = False

    async def respond_to_vacancy(self, vacancy_id: int, resume: Resume) -> Dict[str, str | bool]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—Ç–∫–ª–∏–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é –∏—Å–ø–æ–ª—å–∑—É—è —É–∫–∞–∑–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ."""
        url = "https://hh.ru/applicant/vacancy_response/popup"
        payload = {
            "resume_hash": resume.hash,
            "vacancy_id": str(vacancy_id),
            "lux": "true",
            "ignore_postponed": "true",
            "mark_applicant_visible_in_vacancy_country": "false",
        }
        
        form_data = FormData()
        for key, value in payload.items():
            form_data.add_field(key, value)

        headers = {
            "User-Agent": "Mozilla/5.0",
            "X-Xsrftoken": self.cookies.get("_xsrf", ""),
            "Accept": "application/json",
            "Cookie": cookies_to_string(self.cookies),
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, data=form_data, headers=headers) as response:
                text = await response.text()
                
                if response.status == 403:
                    print(f"403 Forbidden: {text[:100]}")
                    if not self.is_token_being_updated:
                        self.prompt_cookies_update()
                    else:
                        while self.is_token_being_updated:
                            await asyncio.sleep(5)
                    return await self.respond_to_vacancy(vacancy_id, resume)
                
                try:
                    data = json.loads(text)
                except json.JSONDecodeError:
                    print(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON-–æ—Ç–≤–µ—Ç. –°—Ç–∞—Ç—É—Å: {response.status}, –¢–µ–∫—Å—Ç: {text}")
                    return {"success": False, "error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON-–æ—Ç–≤–µ—Ç"}

                if "error" in data:
                    return {"success": False, "error": data["error"]}
                
                if data.get("type") == "need-login":
                    if not self.is_token_being_updated:
                        self.prompt_cookies_update()
                    else:
                        while self.is_token_being_updated:
                            await asyncio.sleep(5)
                    return await self.respond_to_vacancy(vacancy_id, resume)
                
                success_result = {"success": data.get("success") == "true"}
                if success_result["success"]:
                    success_result["resume_used"] = resume.query
                return success_result

class AccountResumePair:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—ã –∞–∫–∫–∞—É–Ω—Ç-—Ä–µ–∑—é–º–µ."""
    
    def __init__(self, account: Account, resume: Resume, pair_id: int):
        self.account = account
        self.resume = resume
        self.pair_id = pair_id
        self.is_exhausted = False

def display_accounts_info(accounts: List[Account]) -> None:
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–∫–∫–∞—É–Ω—Ç–∞—Ö –∏ –∏—Ö —Ä–µ–∑—é–º–µ."""
    print("\n=== –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û–ë –ê–ö–ö–ê–£–ù–¢–ê–• ===")
    for i, account in enumerate(accounts, 1):
        print(f"\n–ê–∫–∫–∞—É–Ω—Ç {i}: {account.email}")
        for j, resume in enumerate(account.resumes, 1):
            blacklist_info = f" (–∏—Å–∫–ª—é—á–µ–Ω–∏—è: {', '.join(resume.blacklist)})" if resume.blacklist else ""
            print(f"  –†–µ–∑—é–º–µ {j}: {resume.query}{blacklist_info}")

def get_search_order_from_user(all_search_queries: List[str]) -> List[str]:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    print("\n=== –í–´–ë–û–† –ü–û–†–Ø–î–ö–ê –û–¢–ö–õ–ò–ö–ê –ù–ê –í–ê–ö–ê–ù–°–ò–ò ===")
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:")
    
    for i, query in enumerate(all_search_queries, 1):
        print(f"{i}. {query}")
    
    print("\n–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ")
    print("–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–∏–º–∏—Ç–∞ –≤ 200 –æ—Ç–∫–ª–∏–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π –∞–∫–∫–∞—É–Ω—Ç.")
    print("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 2 1 3)")
    print("–ò–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞")
    
    while True:
        user_input = input("\n–í–∞—à –≤—ã–±–æ—Ä: ").strip()
        
        if not user_input:
            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –≤—ã–±—Ä–∞–ª –ø–æ—Ä—è–¥–æ–∫, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫")
            return all_search_queries
        
        try:
            # –ü–∞—Ä—Å–∏–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            selected_indices = [int(x) - 1 for x in user_input.split()]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
            if len(selected_indices) != len(all_search_queries):
                print(f"–û—à–∏–±–∫–∞: –ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å {len(all_search_queries)} –Ω–æ–º–µ—Ä–æ–≤")
                continue
                
            if any(i < 0 or i >= len(all_search_queries) for i in selected_indices):
                print(f"–û—à–∏–±–∫–∞: –ù–æ–º–µ—Ä–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç 1 –¥–æ {len(all_search_queries)}")
                continue
                
            if len(set(selected_indices)) != len(selected_indices):
                print("–û—à–∏–±–∫–∞: –ù–æ–º–µ—Ä–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è")
                continue
            
            # –°–æ–∑–¥–∞–µ–º —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            ordered_queries = [all_search_queries[i] for i in selected_indices]
            
            print("\n–í—ã–±—Ä–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫:")
            for i, query in enumerate(ordered_queries, 1):
                print(f"{i}. {query}")
            
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            confirm = input("\n–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å? (y/n): ").strip().lower()
            if confirm in ['y', 'yes', '–¥–∞', '']:
                return ordered_queries
            else:
                print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑")
                continue
                
        except ValueError:
            print("–û—à–∏–±–∫–∞: –í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: 2 1 3)")
            continue

async def get_vacancies_data(session: aiohttp.ClientSession, params: str) -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö —Å —Å–∞–π—Ç–∞."""
    url = f"https://hh.ru/search/vacancy?{params}"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "*/*",
        "X-Static-Version": website_version,
        "X-Xsrftoken": "1",
    }
    
    async with session.get(url, headers=headers) as response:
        text = await response.text()
        match = re.search(r'{"topLevelSite".*"action":"POP"}}', text)
        if not match:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö –∏–∑ –æ—Ç–≤–µ—Ç–∞")
        return json.loads(match.group(0))

async def get_vacancies(session: aiohttp.ClientSession, request: str, page: int) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    params = f"text={request}&salary=&ored_clusters=true&experience=between1And3&page={page}"
    data = await get_vacancies_data(session, params)
    return data["vacancySearchResult"]["vacancies"]

async def get_vacancies_pages(session: aiohttp.ClientSession, request: str) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏."""
    params = f"text={request}&salary=&ored_clusters=true&experience=between1And3&page=1"
    data = await get_vacancies_data(session, params)
    paging = data["vacancySearchResult"]["paging"]
    if paging == None:
        return 1
    else:
        return data["vacancySearchResult"]["paging"]["lastPage"]["page"]

def is_vacancy_blacklisted(vacancy_name: str, blacklist: List[str]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏—Å–∫–ª—é—á–∞–µ–º—ã–µ —Å–ª–æ–≤–∞."""
    if not blacklist:
        return False
    
    vacancy_name_lower = vacancy_name.lower()
    return any(word.lower() in vacancy_name_lower for word in blacklist)

async def process_vacancy(
    vacancy: Dict,
    relevant_pairs: List[AccountResumePair],  # –¢–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    exhausted_pairs: List[int],
    pair_lock: asyncio.Lock,
    pair_index: List[int]
) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏—é –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—Ç–∫–ª–∏–∫, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ."""
    name = vacancy["name"]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º blacklist –¥–ª—è –ø–µ—Ä–≤–æ–π –Ω–∞–π–¥–µ–Ω–Ω–æ–π –ø–∞—Ä—ã (—É –≤—Å–µ—Ö –ø–∞—Ä –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π query –∏ blacklist)
    if relevant_pairs and is_vacancy_blacklisted(name, relevant_pairs[0].resume.blacklist):
        print(f"–í–∞–∫–∞–Ω—Å–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ (blacklist): {name}")
        return

    async with pair_lock:
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∏—Å—á–µ—Ä–ø–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö
        available_pairs = [pair for pair in relevant_pairs if pair.pair_id not in exhausted_pairs]
        
        if not available_pairs:
            print(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–∫–ª–∏–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é: {name}")
            return
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –ø–∞—Ä—É –ø–æ –∫—Ä—É–≥–æ–≤–æ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É —Å—Ä–µ–¥–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö
        pair = available_pairs[pair_index[0] % len(available_pairs)]
        curr_pair_id = pair.pair_id
        pair_index[0] = (pair_index[0] + 1) % len(available_pairs)

    resp = await pair.account.respond_to_vacancy(vacancy["vacancyId"], pair.resume)
    if resp["success"]:
        print(f"–û—Ç–∫–ª–∏–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é: {name} (—Ä–µ–∑—é–º–µ: {pair.resume.query}, –∞–∫–∫–∞—É–Ω—Ç: {pair.account.email})")
    else:
        error = resp["error"]
        if error == "negotiations-limit-exceeded":
            print(f"–õ–∏–º–∏—Ç –æ—Ç–∫–ª–∏–∫–æ–≤ –∞–∫–∫–∞—É–Ω—Ç–∞ {pair.account.email} –∏—Å—á–µ—Ä–ø–∞–Ω.")
            async with pair_lock:
                if curr_pair_id not in exhausted_pairs:
                    exhausted_pairs.append(curr_pair_id)
        elif error != "unknown":
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫–ª–∏–∫–Ω—É—Ç—å—Å—è –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é {name}: {error}")

async def process_resume_vacancies(
    session: aiohttp.ClientSession,
    search_query: str,
    relevant_pairs: List[AccountResumePair],  # –¢–æ–ª—å–∫–æ –ø–∞—Ä—ã, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    exhausted_pairs: List[int],
    pair_lock: asyncio.Lock,
    pair_index: List[int]
) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
    print(f"\n=== –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {search_query} ===")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    available_pairs = [pair for pair in relevant_pairs if pair.pair_id not in exhausted_pairs]
    if not available_pairs:
        print(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {search_query}")
        return
    
    print(f"–î–æ—Å—Ç—É–ø–Ω–æ –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –¥–ª—è '{search_query}': {len(available_pairs)}")
    for pair in available_pairs:
        print(f"  - {pair.account.email}")
    
    last_page = await get_vacancies_pages(session, search_query)
    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è '{search_query}': {last_page}")
    
    for page in range(0, last_page + 1):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—ã –ø–µ—Ä–µ–¥ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
        available_pairs = [pair for pair in relevant_pairs if pair.pair_id not in exhausted_pairs]
        if not available_pairs:
            print(f"\n‚ùå –õ–∏–º–∏—Ç –≤—Å–µ—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{search_query}' –∏—Å—á–µ—Ä–ø–∞–Ω.")
            break
            
        vacancies = await get_vacancies(session, search_query, page)
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}/{last_page} –¥–ª—è '{search_query}' ({len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π)")
        
        tasks = [
            process_vacancy(vacancy, relevant_pairs, exhausted_pairs, pair_lock, pair_index)
            for vacancy in vacancies
        ]
        await asyncio.gather(*tasks)
    
    print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {search_query}")

def cookies_to_string(cookies: Dict[str, str]) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∫—É–∫ –≤ —Å—Ç—Ä–æ–∫—É."""
    return "; ".join(f"{key}={value}" for key, value in cookies.items())

def parse_cookies(cookie_str: str) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –∫—É–∫ –≤ —Å–ª–æ–≤–∞—Ä—å."""
    return dict(cookie.strip().split("=", 1) for cookie in cookie_str.split(";"))

def get_website_version() -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ—Ä—Å–∏—é —Å–∞–π—Ç–∞ hh.ru."""
    url = "https://hh.ru/?hhtmFrom=resume_list"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    }
    response = requests.get(url, headers=headers)
    version = re.search(r"[1-9]{0,2}\.[1-9]{0,2}\.[1-9]{0,2}\.[1-9]{0,2}", response.text)
    if not version:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–µ—Ä—Å–∏—é —Å–∞–π—Ç–∞")
    return version.group(0)

async def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã."""
    try:
        with open(ACCOUNTS_FILE, "r", encoding="utf-8") as file:
            accounts_data = json.load(file)
    except FileNotFoundError:
        print(f"–§–∞–π–ª {ACCOUNTS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    # –°–æ–∑–¥–∞–µ–º –∞–∫–∫–∞—É–Ω—Ç—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    accounts = []
    all_search_queries = set()
    
    for account_data in accounts_data:
        resumes = [
            Resume(
                hash=resume["hash"], 
                query=resume["search_criteria"]["query"],
                blacklist=resume["search_criteria"].get("exclude_words", [])
            ) 
            for resume in account_data["resumes"]
        ]
        if resumes:  # –°–æ–∑–¥–∞–µ–º –∞–∫–∫–∞—É–Ω—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—é–º–µ
            accounts.append(Account(email=account_data["email"], resumes=resumes))
            for resume in resumes:
                all_search_queries.add(resume.query)

    if not accounts:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞–∫–∫–∞—É–Ω—Ç–æ–≤ —Å —Ä–µ–∑—é–º–µ.")
        return

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–∫–∫–∞—É–Ω—Ç–∞—Ö
    display_accounts_info(accounts)

    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã –∞–∫–∫–∞—É–Ω—Ç-—Ä–µ–∑—é–º–µ
    account_resume_pairs = []
    pair_id = 0
    
    for account in accounts:
        for resume in account.resumes:
            account_resume_pairs.append(AccountResumePair(account, resume, pair_id))
            pair_id += 1

    if not account_resume_pairs:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–∞—Ä –∞–∫–∫–∞—É–Ω—Ç-—Ä–µ–∑—é–º–µ.")
        return

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    all_search_queries_list = list(all_search_queries)
    ordered_search_queries = get_search_order_from_user(all_search_queries_list)

    print(f"\n=== –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò ===")
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(account_resume_pairs)} –ø–∞—Ä –∞–∫–∫–∞—É–Ω—Ç-—Ä–µ–∑—é–º–µ")
    print(f"–ü–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: {' ‚Üí '.join(ordered_search_queries)}")

    exhausted_pairs: List[int] = []
    pair_lock = asyncio.Lock()

    session_headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "X-Static-Version": website_version,
        "X-Xsrftoken": "1",
    }

    async with aiohttp.ClientSession(headers=session_headers) as session:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –ø–æ—Ä—è–¥–∫–µ
        for search_query in ordered_search_queries:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É
            relevant_pairs = [
                pair for pair in account_resume_pairs 
                if pair.resume.query == search_query
            ]
            
            if not relevant_pairs:
                print(f"–ù–µ—Ç –ø–∞—Ä –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {search_query}")
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—ã
            available_pairs = [pair for pair in relevant_pairs if pair.pair_id not in exhausted_pairs]
            if not available_pairs:
                print(f"–í—Å–µ –∞–∫–∫–∞—É–Ω—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{search_query}' –∏—Å—á–µ—Ä–ø–∞–Ω—ã.")
                continue
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            pair_index = [0]
            
            await process_resume_vacancies(
                session, search_query, relevant_pairs, 
                exhausted_pairs, pair_lock, pair_index
            )

    print(f"\nüéâ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê!")

if __name__ == "__main__":
    website_version = get_website_version()
    asyncio.run(main())